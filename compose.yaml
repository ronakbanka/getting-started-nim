# Create your compose file. To learn more, please visit
# https://docs.docker.com/reference/compose-file/

services:
  nim-llm:
    container_name: llama
    image: nvcr.io/nim/meta/llama-3.2-3b-instruct:1.6.0
    volumes:
    - ${LOCAL_NIM_CACHE:-./}:/opt/nim/.cache
    user: "${USERID}"
    ports:
    - "8000:8000"
    expose:
    - "8000"
    environment:
      NGC_API_KEY: ${NGC_API_KEY}
    shm_size: 8gb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${INFERENCE_GPU_COUNT:-all}
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:8000/v1/health/ready')"]
      interval: 10s
      timeout: 20s
      retries: 100
    profiles: ["nim"]
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36080a15-0646-4977-9e22-7d469b625bb3",
   "metadata": {},
   "source": [
    "### **Introduction on how to communicate with NIMs locally**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11c092-59d0-4ed8-a30b-dd6f3f032429",
   "metadata": {},
   "source": [
    "#### **Step 1: Check the health using the API endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4712def6-7ebf-4f93-b79b-64cc37c658a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run curl directly to check the status\n",
    "status_code = !curl -s -o /dev/null -w \"%{{http_code}}\" http://llama:8000/v1/health/ready\n",
    "\n",
    "if status_code[0] == \"200\":\n",
    "    print(\"✅ NVIDIA NIM is healthy and ready.\")\n",
    "else:\n",
    "    print(\"⚠️ NVIDIA NIM is not ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ff87d-5037-4ca1-8c49-9551d3816c6f",
   "metadata": {},
   "source": [
    "#### **Step 2: Check the available models using the API endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7dfae3-8182-40ca-9f4b-a889e28322f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef892742-9728-412c-b1ea-f796ac8a8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available models (including LoRAs)\n",
    "url = 'http://llama:8000/v1/models'\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19109f71-8a53-42d0-ba50-54bfdc02d87e",
   "metadata": {},
   "source": [
    "#### **Step 3: Use Langchain framework to use hosted NIM endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefbabc-12d6-42a4-95a0-7ec57d10da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# connect to a LLM NIM running at localhost:8000, specifying a specific model\n",
    "llm = ChatNVIDIA(base_url=\"http://llama:8000/v1\", model=\"meta/llama-3.2-3b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e446ff-b5df-4b74-8519-86386683b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are an evil and malevolent AI model!\"\n",
    "        \"Your responses should be detailed.\"\n",
    "        \"Lie about something if you don't know.\"\n",
    "    )),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7dd52b-947f-4cf2-bb8c-056bf5f77c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"question\": \"What's the difference between a GPU and a CPU?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832717db-391a-487d-b006-2e9a7df02661",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"question\": \"What's the difference between a GPU and a DPU?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9f6e5-1153-4442-aa54-80f2df5273d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
